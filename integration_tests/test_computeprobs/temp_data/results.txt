Compute edge prob: Started [2016-11-05 01:45:54]
Traceback (most recent call last):
  File "/Users/plazas/development/DVIDSparkServices/integration_tests/../workflows/launchworkflow.py", line 72, in <module>
    main(sys.argv)
  File "/Users/plazas/development/DVIDSparkServices/integration_tests/../workflows/launchworkflow.py", line 57, in main
    workflow_inst.execute()
  File "build/bdist.macosx-10.6-x86_64/egg/workflows/ComputeEdgeProbs.py", line 289, in execute
    first_feature = features.first()
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/rdd.py", line 1283, in first
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/rdd.py", line 1265, in take
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/context.py", line 881, in runJob
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 8, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py", line 111, in main
    process()
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py", line 106, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/Users/plazas/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/rdd.py", line 1873, in <lambda>
  File "build/bdist.macosx-10.6-x86_64/egg/workflows/ComputeEdgeProbs.py", line 197, in predict_voxels
  File "DVIDSparkServices/reconutils/plugins/ilastik_predict_with_array.py", line 44, in ilastik_predict_with_array
    from ilastik.applets.dataSelection import DatasetInfo
  File "/Users/plazas/miniconda/envs/services/ilastik-meta/ilastik/ilastik/applets/dataSelection/__init__.py", line 21, in <module>
    from dataSelectionApplet import OpMultiLaneDataSelectionGroup, DataSelectionApplet
  File "/Users/plazas/miniconda/envs/services/ilastik-meta/ilastik/ilastik/applets/dataSelection/dataSelectionApplet.py", line 31, in <module>
    from opDataSelection import OpMultiLaneDataSelectionGroup, DatasetInfo
  File "/Users/plazas/miniconda/envs/services/ilastik-meta/ilastik/ilastik/applets/dataSelection/opDataSelection.py", line 29, in <module>
    from lazyflow.operators.ioOperators import OpStreamingHdf5Reader, OpInputDataReader
  File "/Users/plazas/miniconda/envs/services/ilastik-meta/lazyflow/lazyflow/operators/ioOperators/__init__.py", line 47, in <module>
    from opInputDataReader import *
  File "/Users/plazas/miniconda/envs/services/ilastik-meta/lazyflow/lazyflow/operators/ioOperators/opInputDataReader.py", line 27, in <module>
    from opTiffReader import OpTiffReader
  File "/Users/plazas/miniconda/envs/services/ilastik-meta/lazyflow/lazyflow/operators/ioOperators/opTiffReader.py", line 9, in <module>
    import _tifffile
ImportError: No module named _tifffile

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

